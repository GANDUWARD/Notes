# Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads


## 分布式训练策略：分布式训练策略在这篇文章中包含了以下工作：


## worker：负责前向传播和反向传播计算


## parameter server：负责模型参数更新

![alt text](<论文图片/Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads/Harmony workflow.png>)
**离线训练**：通过深度强化学习神经网络模型来减少训练所需的样本量


**在线推理**：对每次新到达的作业用模型进行放置决策。


**模型更新**：定期对模型进行更新与训练

## Interference Aware Scheduler：**干扰感知调度器**，比市面上的YARN等弱于干扰感知调度器在如今分布式训练的场景中更为被需要




当今的**白盒方法**主要面向特定场景和执行，对于干扰源有数十种明确的靶向并且依赖于对模型的细致优化与启发式方法的相关阈值

因为任何预设策略和数学模型的方法都会存在**通用性问题**

面对如今的场景，我们可以采用**基于深度学习的神经网络模型的负载均衡器**，用基于海量经验数据的**黑盒方法**，来对分布式大模型实现负载均衡和任务分配策略，工作感知的动态空间探索，经验再现


1. **负载均衡（Load Balancing）**： 负载均衡旨在将**计算任务均匀地分配到集群中的各个节点**上，以避免某些节点过载，而其他节点闲置。​在机器学习集群中，负载均衡可以通过监控各节点的资源使用情况，动态调整任务分配策略，从而提高整体资源利用率和任务处理效率。​

2. **装箱（Bin Packing）**： 装箱问题源自将**物品尽可能高效地放入有限容量的容器中**的经典优化问题。​在作业调度中，装箱方法关注如何将多个具有不同资源需求的任务安排到有限的计算资源上，以最小化资源浪费。​这通常涉及到复杂的优化算法，旨在在满足任务需求的同时，最大化资源利用率。​

3. **独立（Standalone）**： 在某些情况下，某些任务可能需要**独占资源运行**，或者由于其特殊的资源需求和性能要求，不能与其他任务共享节点资源。​这种情况下，任务以独立模式运行，确保其性能和稳定性。

## DRL：深度强化学习，其训练方法包括演员-评论家算法，

**分配策略**会有这样的问题：行为空间在jobs种类数量以及worker还有参数服务器还有服务器中随着数量**指数级增长**：哪怕六个工作在基于3个workers加上参数服务器以及六个服务器有超过一亿种放置方式


**Harmony**：机器学习集群调度器。以批处理方式调度作业，时间被划分为小的调度间隔。Harmony**不假设**作业到达时间是**预先已知**的，并在每个间隔中**批量处理新到达的作业**。然后它**决定整个批次的作业放置**，即每个作业中的每个工作者和每个参数服务器应该在哪个服务器上运行（作为虚拟机或容器）。然后根据放置决策部署作业，并且Harmony运行这些作业直至完成，即每个作业的工作者和参数服务器的放置在整个训练过程中**不会改变**。因此，Harmony只在每个调度间隔中**调度新到达**的作业，根据当前的资源可用性和服务器上先前到达作业的现有放置情况来放置它们。根据我们与运营大型AI云的公司（例如，微软[50]和阿里巴巴[51]）的讨论，**带有资源规格说明且部署后不进行放置调整的作业提交是机器学习集群中的常态**，主要是由于两个原因：（1）停止训练作业然后重新开始需要修改机器学习框架和用户代码以恢复正确的训练状态（例如，已训练的周期和迭代次数、随机种子等）。（2）停止和重新开始训练作业会带来显著的开销，包括重新调度时间、容器重启、数据集重新加载、检查点重新加载等[52]。在实践中，对正在运行的作业进行**无重大开销的动态资源调整**是难以实现的。






**样本量不足问题**：即使是简单的DeepRM——仅有一个隐藏层的简单神经网络也要用20000个样本。要解决这个问题，就需要采用一个基于神经网络的奖励预测模型



## 状态空间

**输入状态序列s**:由（s1,s2...sn）组成sn个数表示当前并发的作业数。sn由(xn, rn, wn, pn, v, d
n, un)


**xn**:xn,  L维二进制向量，编码了由作业n训练的ML模型，其中L是集群中可以训练的最大模型数量（即所有时间点上训练作业的类型总数）。为了简化，每个向量xn是一个作业n的模型的一次性编码[39]。相同的ML模型，例如，具有相同架构和小批量大小的DNN，但可能具有不同的学习率和总训练周期数，使用相同的编码。例如，如果有3个模型总共，并且每个模型分别使用3个并发作业，那么x0 = [1, 0, 0]，x1 = [0, 1, 0]，x2 = [0, 0, 1]。还存在其他可能的编码方法，例如特征嵌入[53]，我们将探索更高效的编码方法作为未来的工作。

**rn**:2(1+K)维度向量，K是资源种类数，第一个值表示n号job请求worker个数，后面K个值代表每个worker的各种资源请求个数。最后1+K表示job所请求的参数服务器个数以及每个参数服务器各个资源请求个数


**wn**:整数表示worker的数量



**v**:，一个 M × K 矩阵，代表服务器上每种类型资源的可用量，其中 M 是物理服务器的数量。每个向量 vm，∀m = 1,...,M，代表服务器 m 上的可用资源。例如，一个有 8 个可用 CPU 核心和 2 个可用 GPU 的服务器被编码为 vm = [8, 2]。

**dn**:一个M*2的向量对jobn的所有服务器上的worker和参数服务器进行编码服务器m在其中位置2m-1表示worker数，2m表示参数服务器数量，全归约结构中，参数服务器为0，所以2m位置肯定是0



**un**： 一个整数，表示第n个作业是新到达的还是在之前的调度间隔中已经安排好的。例如，u0 = 0 和 u1 = 1 分别表示作业0已经在之前被安排过，而作业1是当前调度间隔中到达的新作业。





## 动作空间


**πθ(s, a)**: 根据参数θ来对输入状态序列s，和读取决策空间a，来确定放置策略。只针对刚到达的job一个个作出决策，（0，m）表示对于新的job一个worker将其放到服务器m上，（1，m）表示对于新job一个参数服务器将其放到服务器m上。（全规约模型要手动把可能性设置为0并且调整其他非零概率使其为1）

# 奖励机制


我们通过训练策略神经网络（NN），旨在最小化平均作业完成时间，该网络学习如何提高资源利用率和减轻作业间的干扰。作业完成时间是一个自然的奖励观察对象，但只有在作业完成后才知道，这可能是在数百个调度间隔之后。由于奖励的显著反馈延迟对于训练是不可接受的，因为延迟的奖励对改善早期决策的指导作用很小。此外，一个作业的完成时间不仅由当前的作业放置状态决定，还由未来即将部署的作业（可能在相同的服务器上并干扰这个作业）决定。我们设计了一个每间隔的奖励，通过作业过程收集更多的奖励样本，以便更频繁地更新强化学习（RL）模型，以加快收敛速度。

**奖励r**  当行为a在状态s下被采取时候被观察到。r为各个作业的（实际训练迭代次数除以应训练迭代次数）求和

# 表征网络

**前向传播NN**一般只能处理固定大小的输入，如果要处理大小不定的输入，则必须设定作业的并发量上限并且填充输入序列。这样预定义的大小如果过大会造成资源冗余，过小会因为剔除输入作业而造成状态信息输入的损失。因此可以采用transformer架构的输入方式将输入信息保存在一个固定尺寸矩阵序列中。






**Transformer的注意力架构**：注意力机制模型对输入顺序不敏感



# 编码网络



# DRL 模型训练

我们应用REINFORCE算法[57]来训练策略神经网络（NN），该算法使用通过样本计算得到的策略梯度来更新神经网络参数θ。每个样本是一个四元组，(s, a, r, s' )，其中s' 是在状态s中采取行动a后的新状态。请注意，我们的系统与标准的强化学习（RL）运行方式不同：在每个调度间隔t中，我们使用NN进行多次推理（即产生多个动作）；每次推理后输入状态都会改变；我们只观察到奖励，并在间隔内的所有推理完成后更新NN一次。设It为间隔t中的推理集合；那么我们可以在间隔内获得It个样本，并且我们将这些样本中的每个奖励设置为在t内所有推理完成后观察到的奖励（1）。


**累计折扣奖励J（θ）** ：J=E[sum（ i≥0， γi*ri）],其中γ表示折扣因数，i表明从系统开始时推理完成的总数，ri表示第i次推理所对应的奖励。对于间隔t，J的梯度计算公式如下：




![alt text](<论文图片/Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads/J(θ) policy.png>)

Q值，**Qπθ(si, ai)**，代表了在给定状态si下按照策略πθ采取动作ai的“质量”，计算方式为在遵循πθ策略下，从状态si选择动作ai后预期获得的累积折扣奖励的期望值，即， 


![alt text](<论文图片/Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads/Q policy.png>)

由于我们无法枚举所有可能的未来状态来计算一个精确的Q值，我们可以使用一个样本小批量来计算经验Q值[57]，然后计算梯度θJ(θ)。




# 稳定训练，加速策略收敛，并提高所获得策略的质量的方法



**演员-评论家**：强化算法可能因导出的Q值（用于计算梯度）方差过高而影响策略模型的快速收敛[58]。为了减少方差，我们改进了基于演员-评论家算法的基本策略梯度训练[47]。基本思路是引入一个依赖于状态的基线函数，以提高用于更新策略神经网络的SGD中的梯度。如果某个动作“质量”Qπθ（si，ai）优于该状态下所有可能动作的“平均质量”V_πθ（si），则从该状态强化该动作。这里，“平均质量”是指根据策略π从状态si出发，在所有可能动作下预期累积奖励的总和。也就是说，我们通过其优势来评估一个动作的好坏，即Qπθ（si，ai）−V_πθ（si）。我们在公式(2)中使用这个优势而不是Qπθ（si，at）来进行梯度计算。目的是确保策略梯度估计的方差大大降低，从而使策略学习更加稳定。价值函数V_πθ（si）通常由价值网络估计。它具有与策略网络相同的架构，只是输出层是一个线性神经元，没有任何激活函数。



**作业感知探索**：为了通过DRL获得良好的策略，我们需要确保动作空间得到充分探索（即，能够产生高奖励的动作足够多）；否则，DRL可能会收敛到较差的局部最优策略[39]，[58].一种鼓励探索的方法是在公式(2)中的梯度计算中加入熵正则化项β
![alt text](<论文图片/Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads/β policy.png>)
其中β是熵权重。熵正则化的基本思想是鼓励动作概率均匀分布，防止收敛到单一输出选择。
然而，我们发现由于作业放置的巨大探索空间，仅靠熵正则化是不够的。我们进一步采用了基于-贪婪方法[47]的另一种技术。在对策略神经网络进行每次推理时（用于其训练），以概率1−，采用根据神经网络输出策略分布产生的动作（工作节点/参数服务器放置决策）；以概率，随机选择多资源箱装和负载均衡策略，并采用所选策略产生的放置决策。装箱策略将一个工作进程或参数服务器（在NN选择的操作中指定）放置在剩余容量最少的机器上（在此机器上仍可容纳该工作进程或参数服务器）；负载均衡将一个工作进程或参数服务器放置在负载最小的机器上。其目的是让NN有效地探索资源利用率（对装箱最好的）和工作负载干扰（负载均衡所避免）之间的权衡，从而提高探索质量，引导NN训练收敛到一个好的策略




**多头注意力机制**：该注意力机制旨在捕捉输入作业序列中的依赖关系，通过计算所有作业信息对的点积，并使用注意力函数，然后计算输入序列中并发作业信息的加权和。一个注意力层可以将依赖关系映射到一个序列中，每个序列包含较小的向量。此外，我们并行地使用不同的注意力函数投影作业信息，从而产生多维输出。多头注意力机制设计用于让表示网络能够同时关注来自不同位置的不同表示子空间的信息。我们在实验中使用了8个并行注意力层（即，头）。






**经验回放**：连续样本训练强化学习模型难以收敛，因为样本之间存在相关性[41]。当前的策略神经网络决定了以下训练样本，例如，如果策略网络发现打包作业能提高奖励，那么下一个样本序列将主要由这种策略产生的样本主导；这可能导致**不良反馈循环**，阻碍探索高奖励的样本。我们采用经验回放[41]来**缓解样本序列中的相关性**。具体来说，我们维护一个固定大小（例如，在我们的实验中为8192）的**先进先出回放缓冲区**，足够大以缓冲来自多个调度间隔的样本。在每个调度间隔计算策略神经网络更新的梯度时，我们不是使用该间隔内收集的所有样本，而是从回放缓冲区中随机选择一个小型样本批次（例如，在我们的实验中为32个样本），这些样本可能来自多个先前的间隔。





# 奖励预测模型：采用神经网络作为奖励模型





**神经网络架构**。输入到神经网络的状态是DRL神经网络输入的一个子集：序列s =（s1，...，sN），其中sn=（xn，wn，pn，dn），包括以下内容：(i)xn，作业n的模型类型。（ii）wn和pn，第n个作业分配的工人数量和参数服务器数量。（iii）dn，作业n在每个服务器上的放置情况。由于通常可以从作业的模型类型推断出工作者和参数服务器的资源需求，因此未包含这些信息。输出是一个向量，包含基于当前放置情况dn（∀n∈[N]）预测的输入作业的训练速度（即，在一个区间内完成的训练轮数）。因为以下原因：在一个作业执行过程中，服务器按部署的worker或参数服务器进行放置情况可能会改变；因为新作业的到来会影响作业完成时间；因此，**考虑到当前集群中的工作位置，预测当前位置下的训练速度比预测最终的工作完成时间更合理**。**所以我们不直接输出预测过的作业完成时间**。神经网络的输入也有非固定的输入尺寸，我们采用序列到序列的Transformer模型来处理输入任务并生成一个序列，即并发任务的预测训练速度。每个任务的输入状态连接到一系列隐藏的全连接层以进行嵌入（图9中的任务/放置嵌入块），然后是一个基于Transformer的编码器，在线性输出层之前。类似于第五节A部分，我们也使用注意力机制来捕捉并发任务之间的相关性。由于输出序列中没有依赖关系，我们可以一次性生成整个输出序列，因此不包括Transformer的解码器部分。实际上，我们发现全连接层在我们的场景中表现得相当好，比更复杂的神经层如卷积层（通常用于图像处理[59])要好得多。详细的架构可以在图9中找到。



![alt text](<论文图片/Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads/Reward Prediction Model Architecture(1).png>)

**神经网络训练**。我们通过监督学习使用历史轨迹中的可用样本来训练神经网络。我们将神经网络预测的每个作业n的训练速度cn与标签c'n进行比较，即轨迹中每个作业n的实际训练速度。通过计算损失函数来实现这一点，该损失函数是预测值与标签之间的相对误差：L（c，c'）=1|N|n∈[N]|c'n−cn|c'n
![lp](/论文/论文图片/label%20policy.png)
然后我们使用随机梯度下降更新神经网络中的参数，以最小化总体相对误差。我们使用历史轨迹中的样本迭代训练神经网络，使得神经网络的预测结果收敛到可接受的相对误差（例如，10%。






# 性能评估

## 实现


**k8s上调度器**：我们使用python在Kubernetes 1.7 [23]上实现Harmony作为自定义调度器。我们在**docker容器中运行工作节点和参数服务器**。作业的**训练数据集存储在HDFS 2.8** [60]中。在调度间隔开始时，Harmony通过向Kubernetes API服务器发送HTTP请求查询未调度的作业和现有的集群状态，并通过训练好的策略网络进行推理来做出这些作业的放置决策。每个推理在GTX 1080Ti GPU上需要4 ms的时间。Kubernetes代理根据服务器启动每个作业的工作者/参数服务器。Harmony**使用在线收集的数据**更新DRL和奖励模型。我们使用**MXNet框架[2]运行每个训练作业**。




**DRL训练。**对于离线训练，我们使用TensorFlow [1]提供的库实现DRL卷积神经网络。第一个嵌入层的节点数与序列或解码器输入中每个输入条目的大小相同。表示网络有2个隐藏层，每个隐藏层包含128个节点和8个注意力头。解码器神经网络也有2个隐藏层，每层包含128个神经元。我们并行训练DRL卷积神经网络：使用20个工作进程生成样本（利用奖励预测模型）并在本地计算梯度；同步聚合梯度以获得全局参数。我们采用Adam优化器[61]，固定学习率为0.0001，每个工作进程的小批量大小为32个样本，奖励折扣因子γ = 0.9，经验回放缓冲区大小为8192个样本。贪婪探索因子和熵权重β初始设置为0.5，并在训练过程中线性退火。






**奖励模型训练**。我们使用TensorFlow实现奖励神经网络（NN）[1]。奖励NN具有2个隐藏编码层，每层有128个神经元，并且在Transformer结构中有8个注意力头。我们使用Adam优化器[61]进行训练，伴随一个可变的学习率，并且批量大小为32个样本。我们还对奖励NN应用批量归一化以加速收敛[59]。 
奖励模型的追踪数据是在我们的GPU集群上收集的：我们生成具有随机资源配置的工作，随机放置它们，并测量每个工作的训练速度；生成的工作数量及其工作器/参数服务器配置足以饱和我们集群的资源容量。我们将所有减少视为一个特殊情况，即一个工作器和一个参数服务器被捆绑在一起。 
我们运行每个工作大约5分钟，并计算平均训练速度。由于模型训练的迭代性，运行5分钟应该足以给我们一个大致的训练速度概念。




## 评估方法学


**测试平台**我们构建了一个由6台GPU服务器组成的测试平台，这些服务器通过戴尔网络Z9100-ON 100GbE交换机连接。每台服务器配备一个8核英特尔E5-1660 CPU，两个GTX 1080Ti GPU，48GB内存，一个MCX413A-GCAT 50GbE网卡，一个480GB固态硬盘和一个4TB机械硬盘。我们部署了Kubernetes 1.7作为集群管理器。




**工作负载**：默认情况下，任务以均匀随机的方式提交到集群中，平均每间隔提交3个任务。每个间隔持续20分钟。当到达事件发生时，我们从表I中随机选择一个任务，并在[1,3]中随机设置其所需的工作节点数和/或参数服务器数，生成一个任务变体。有0.5的概率选择参数服务器架构，同样有0.5的概率选择全归约架构。对于训练大型数据集的任务（例如，ImageNet [62])，我们会缩小数据集的规模，以便在合理的时间内完成训练。作业长度遵循一个从生产集群（在这里有大量的短作业和一些长作业）提取的偏态分布。我们为harmony与**基线**采用相同作业序列与他们的到达时间




**基线**：我们依照负载均衡（LB）、俄罗斯方块、提瑞西阿斯、最小干扰优先的策略


**负载均衡**：根据最小的负载为worker或参数服务器分配物理机。通过将CPU或GPU资源规范化然后求和作为一个物理服务器的负载


**俄罗斯方块**：它采用多种资源装箱策略来放置worker或参数服务器以免资源碎片化


**提瑞西阿斯**：它提出了2D-Gittins指数来估计作业完成时间并且使用2D-LAS（SRTF变体）来最小化深度学习作业平均作业完成时间。提瑞西阿斯也通过针对作业属性使用扩散或巩固策略来考虑作业放置情况



**最少干扰优先**：它通过假设任务减速是CPU和带宽使用情况的函数，构建了一个线性或非线性干扰模型。为了确定干扰模型中的系数，我们分析了每个机器学习模型在不同CPU和带宽使用情况下的性能，并利用最小二乘法求解器根据分析数据来计算系数。我们发现线性函数和二次函数最符合我们的测量轨迹，因此将其作为两个基准，即LIF-Line和LIF-Quad。当放置一个工作节点/参数服务器时，算法会为每个服务器计算一个干扰分数（即所有工作节点/参数服务器在该服务器上的性能下降），并选择干扰最小的服务器。



**度量**我们使用平均作业完成时间作为主要性能度量。




# 表现




![alt text](<论文图片/Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads/不同作业到达模式的性能对比.png>)



图10比较了Harmony在三种作业到达模式下的性能与基线：(1)**默认均匀分布**；(2)每调度间隔到达率为2的**泊松过程**；(3)从**Google集群跟踪**[64]中提取的作业到达过程，但调整了到达率。Harmony在三种情况下相比所有基线平均作业完成时间（以间隔数计）**提高了16%-42%**。Harmony通过探索找到了负载均衡和分箱打包之间的良好平衡，共同减少了计算干扰和数据传输时间，并通过每次决策后的反馈改进了其调度策略。负载均衡的表现不如Harmony，因为它仅关注减少每台机器上的计算干扰，而**未考虑网络传输开销**。Tetris试图将作业组合在一起以避免资源碎片浪费，并**忽略了由干扰引起的性能下降**。LIF-Line和LIF-Quad**严重依赖于作业间干扰的精确建模**。Tiresias在放置策略中**不考虑CPU或IO干扰**。对于网络干扰，它使用一个阈值来根据参数服务器[31]上的张量分布的偏斜度来确定一个作业是否应该被合并。**如何确定一个好的阈值也是困难的**。


![alt text](<论文图片/Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads/不同作业类型性能对比.png>)


![alt text](<论文图片/Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads/调度间隔的敏感分析.png>)

图11展示了不同类型作业完成时间的细分。我们发现Harmony可以优化**所有类型**的作业，尽管对于某些作业如Seq2Seq，差异不大。我们在图12中通过改变作业调度间隔从10分钟到70分钟进行了敏感性分析。当调度间隔较小时，每个调度区间内的作业较少，可能会**错失一些优化机会**。而当调度间隔较大时，可以在一个批次中处理更多作业，从而优化整体完成时间，但**作业等待时间会更长**。在我们的系统中，调度间隔的最佳点是20分钟。


![alt text](<论文图片/Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads/训练速度预测.png>)


![alt text](<论文图片/Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads/30台以上机器不同作业到达模式下的性能对比.png>)

我们还将奖励模型与干扰模型在LIF-Line和LIF-Quad中进行了比较。我们将收集到的轨迹(29264个样本）打乱，并将其分为**训练数据集（90%）和测试数据集（10%）**。图13(a)显示，我们的模型实现了6.9%的相对误差，远低于LIF-Line和LIF-Quad中的干扰模型。在图13（b）中我们也通过找到相关误差在10%内样本的百分比计算了每种解决方法的精准度。我们可以发现，我们的模型能够确保超过83%的预测结果，其相对误差小于10%。此外，图13(c)展示了两种模型**Seq2Seq**和**CTC**在预测作业速度时产生的相对误差及准确性。我们看到，在LIF-Line和LIF-Quad中，干扰模型在CTC上表现良好，但在Seq2Seq上存在较大误差，原因如下：CTC对CPU需求较高，而Seq2Seq由于预处理开销低且主要在GPU上进行计算；LIF仅构建了CPU和网络共享的干扰模型。图13(d)所示的准确性相似。为了验证Harmony的可扩展性，我们基于分析模型[13]、[63]生成了30台机器（相当于一个机架规模）上的作业放置模拟轨迹，如图14所示。我们使用这些模拟轨迹**训练奖励预测神经网络和我们的DRL神经网络**，并在[1,10]中随机设置所需的工作节点数和/或参数服务器数量。我们观察到，与基线相比，Harmony仍能将平均作业完成时间减少至少21%。







## 深入研究 ：评估神经元、隐藏层、注意力头、价值网络数量、探测方法、多头注意力机制、经验再现。



![alt text](<论文图片/Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads/深入研究.png>)




**神经元数量**。我们在编码器和解码器中将隐藏层的数量固定为2，并改变隐藏层中的神经元数量。我们使用相同的训练集训练所有神经网络直至收敛。图15(a)显示，当**神经元数量为128**时，性能最佳。神经元数量较少时，不足以近似放置策略。过多的神经元会导致性能下降，因为可能会捕捉到不必要的特征（即过拟合）。



**隐藏层数量。**我们在Transformer编码器和解码器的隐藏层中固定神经元数量为128，并改变隐藏层数量。如图15(b)所示，具有**2个隐藏层的神经网络表现最佳**。随着神经网络**层数减少，参数不足以近似出好的策略**；而**层数增加时，通常需要更长时间才能收敛，并且由于过拟合导致性能下降**。


**注意力头的数量**：我们固定神经元数量为128，Transformer编码器和解码器中均包含两层隐藏层，并将注意力头的数量从1到32进行变化。如图15(c)所示，**8个注意力头的神经网络表现最佳**。头越少编码输入的参数不足；头越多会总体提升神经网络复杂度需要更多的试验来训练所有NN参数到合适的值，这导致了**不稳定的训练进度**。

![alt text](<论文图片/Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads/训练方法的效果.png>)


**价值网络**：为了研究价值网络如何影响训练，我们没有使用价值网络来提供基线，而是在训练策略网络时，用奖励的指数移动平均值作为梯度计算的基线。表二中第一行显示了应用所有训练技术后的平均作业完成时间。我们发现，如果没有价值网络，性能会下降33.04%。这是因为平均奖励并不总是有效的基线；在某些情况下，即使最优动作导致的奖励也低于历史上的平均奖励。



**试探**：我们考察探索如何影响性能。从表二中可以看出，没有试探时，性能显著下降（即减速35.54%）。原因是如果没有试探，DRL神经网络可能会进行**大量无效尝试**，尝试许多明显糟糕的动作，从而容易**陷入局部最优策略**。



**多头注意力机制**：我们将注意力头的数量改为一个，考察注意力层数量如何影响性能。从表二中可以看出，性能比使用8个注意力头时更差（即慢了27.36%）。原因是如果没有足够的注意力层，表示网络**很难保留输入状态的必要信息**。



**经验回放**：我们禁用经验回放以检验其在DRL训练中的有效性。如表二所示，没有经验回放时，平均作业完成时间增加了24.71%，这表明**打破样本顺序并在不同调度间隔中使用样本**来更新神经网络对于我们的DRL训练至关重要。



## 结论


## MapReduce jobs：网络密集型
















## HPC jobs：缓存访问密集型

